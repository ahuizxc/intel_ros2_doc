{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intel Ros2 Project Tutorial Overview Intel Ros2 Project contains several ROS2 packages in object classification, detection, localization, tracking, SLAM and navigation. Package Lists ros2_intel_realsense ros2_intel_realsense for using Intel RealSense cameras (D400 series) with ROS2. ros2_object_analytics ros2_object_analytics is a ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. ros2_openvino_toolkit The OpenVINO\u2122 toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the Toolkit extends computer vision (CV) workloads across Intel\u00ae hardware, maximizing performance. This project is a ROS2 wrapper for CV API of OpenVINO\u2122, providing the following features: Support CPU and GPU platforms Support standard USB camera and Intel\u00ae RealSense\u2122 camera Support Video or Image file as detection source Face detection Emotion recognition Age and gender recognition Head pose recognition Object detection Demo application to show above detection and recognitions navigation2 The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way. License Copyright 2018 Intel Corporation Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. * Other names and brands may be claimed as the property of others Any security issue should be reported using process at https://01.org/security","title":"Home"},{"location":"#intel-ros2-project-tutorial","text":"","title":"Intel Ros2 Project Tutorial"},{"location":"#overview","text":"Intel Ros2 Project contains several ROS2 packages in object classification, detection, localization, tracking, SLAM and navigation.","title":"Overview"},{"location":"#package-lists","text":"","title":"Package Lists"},{"location":"#ros2_intel_realsense","text":"ros2_intel_realsense for using Intel RealSense cameras (D400 series) with ROS2.","title":"ros2_intel_realsense"},{"location":"#ros2_object_analytics","text":"ros2_object_analytics is a ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM.","title":"ros2_object_analytics"},{"location":"#ros2_openvino_toolkit","text":"The OpenVINO\u2122 toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the Toolkit extends computer vision (CV) workloads across Intel\u00ae hardware, maximizing performance. This project is a ROS2 wrapper for CV API of OpenVINO\u2122, providing the following features: Support CPU and GPU platforms Support standard USB camera and Intel\u00ae RealSense\u2122 camera Support Video or Image file as detection source Face detection Emotion recognition Age and gender recognition Head pose recognition Object detection Demo application to show above detection and recognitions","title":"ros2_openvino_toolkit"},{"location":"#navigation2","text":"The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way.","title":"navigation2"},{"location":"#license","text":"Copyright 2018 Intel Corporation Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. * Other names and brands may be claimed as the property of others","title":"License"},{"location":"#any-security-issue-should-be-reported-using-process-at-https01orgsecurity","text":"","title":"Any security issue should be reported using process at https://01.org/security"},{"location":"package_instruction/","text":"Quick Start NOTE: Here we just provide a streamlined version to each projects, if you want to learn the details, please refer to their homepages. NOTE: All projects are depend on Intel Reanlsense cameras. If use other RGB-D sensers, you may need to modify the source code according to instructions as follows. ros2_intel_realsense 1 Overview These are packages for using Intel RealSense cameras (D400 series) with ROS2. 2 Running the demo 2.1 Start the camera node To start the camera node in ROS2, plug in the camera, then type the following command: # To launch with \"ros2 run\" $ ros2 run realsense_ros2_camera realsense_ros2_camera # OR, to invoke the executable directly $ realsense_ros2_camera This will stream all camera sensors and publish on the appropriate ROS2 topics. PointCloud2 is enabled by default, till we provide ROS2 python launch options. 2.2 View camera data # in Terminal #1 launch realsense_ros2_camera $ source ~/ros2_ws/install/local_setup.bash $ realsense_ros2_camera # in terminal #2 launch rviz2 $ source ~/ros2_ws/install/local_setup.bash $ rviz2 This will launch RViz2 and display the five streams: color, depth, infra1, infra2, pointcloud. NOTE: Visulization in ROS2 pending on rviz2 . 3 Interfaces /camera/depth/image_rect_raw /camera/color/image_raw /camera/infra1/image_rect_raw /camera/infra2/image_rect_raw /camera/depth/color/points 4 Known Issues This ROS2 node does not currently provide any dynamic reconfigure support for camera properties/presets. We support Ubuntu Linux Xenial Xerus 16.04 on 64-bit, but not support Mac OS X 10.12 (Sierra) and Windows 10 yet. 5 TODO A few features to be ported from the latest realsense_ros_camera v2.0.2 RGB-D point cloud (depth_registered) Preset/Controls ros2_object_analytic 1 Overview Object Analytics (OA) is ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. It consumes sensor_msgs::PointClould2 data delivered by RGB-D camera, publishing topics on object detection , object tracking , and object localization in 3D camera coordination system. OA keeps integrating with various \"state-of-the-art\" algorithms. Object detection offload to VPU, Intel Movidius NCS, with MobileNet SSD model and Caffe framework. 2 Running the demo Step 1. [In terminal 1] Launch Realsense camera node # Terminal 1: . <install-space-with-realsense-ros2-camera>/local_setup.bash realsense_ros2_camera Step 2. [In terminal 1] Launch NCS and OA node # Terminal 2 . <install-space-with-object-analytics-launch>/local_setup.bash echo -e \"param_file: mobilenetssd.yaml\\ninput_topic: /object_analytics/rgb\" > `ros2 pkg prefix movidius_ncs_launch`/share/movidius_ncs_launch/config/default.yaml launch `ros2 pkg prefix object_analytics_launch`/share/object_analytics_launch/launch/analytics_movidius_ncs.py Step 3. [In terminal 1] Launch OA Rviz # Terminal 3 . <install-space-with-object-analytics-launch>/local_setup.bash launch `ros2 pkg prefix object_analytics_launch`/share/object_analytics_launch/launch/object_rviz.py 3 Interfaces 3.1 Subscribed topics /movidius_ncs_stream/detected_objects ( object_msgs::msg::ObjectsInBoxes ) 3.2 Published topics /object_analytics/rgb ( sensor_msgs::msg::Image ) /object_analytics/pointcloud ( sensor_msgs::msg::PointCloud2 ) /object_analytics/localization ( object_analytics_msgs::msg::ObjectsInBoxes3D ) /object_analytics/tracking ( object_analytics_msgs::msg::TrackedObjects ) 3.3 Customize launch By default, object analytics will launch both tracking and localization features, but either tracking or localization or both can be dropped. Detailed please refer comments embedded in launch file. 4 Known issues -- 5 TODO - ros2_openvino_toolkit 1. Overview The OpenVINO\u2122 (Open visual inference and neural network optimization) toolkit provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. By leveraging Intel\u00ae OpenVINO\u2122 toolkit and corresponding libraries, this runtime framework extends workloads across Intel\u00ae hardware (including accelerators) and maximizes performance. Enables CNN-based deep learning inference at the edge Supports heterogeneous execution across computer vision accelerators\u2014CPU, GPU, Intel\u00ae Movidius\u2122 Neural Compute Stick, and FPGA\u2014using a common API Speeds up time to market via a library of functions and preoptimized kernels Includes optimized calls for OpenCV and OpenVX* 2. Running the demo run face detection sample code input from StandardCamera. ros2 launch dynamic_vino_sample pipeline_people_oss.launch.py run face detection sample code input from Image. ros2 launch dynamic_vino_sample pipeline_image_oss.launch.py run object detection sample code input from RealSenseCamera. ros2 launch dynamic_vino_sample pipeline_object_oss.launch.py run object detection sample code input from RealSenseCameraTopic. ros2 launch dynamic_vino_sample pipeline_object_oss_topic.launch.py run object segmentation sample code input from RealSenseCameraTopic. ros2 launch dynamic_vino_sample pipeline_segmentation.launch.py run object segmentation sample code input from Video. ros2 launch dynamic_vino_sample pipeline_video.launch.py run object detection service sample code input from Image Run image processing service: ros2 launch dynamic_vino_sample image_object_server_oss.launch.py Run example application with an absolute path of an image on another console: ros2 run dynamic_vino_sample image_object_client ~/Pictures/car.png 3. Interfaces Subscribed Topic Image topic: /openvino_toolkit/image_raw ( sensor_msgs::msg::Image ) Published Topic Face Detection: /ros2_openvino_toolkit/face_detection ( object_msgs::msg::ObjectsInBoxes ) Emotion Recognition: /ros2_openvino_toolkit/emotions_recognition ( people_msgs::msg::EmotionsStamped ) Age and Gender Recognition: /ros2_openvino_toolkit/age_genders_Recognition ( people_msgs::msg::AgeGenderStamped ) Head Pose Estimation: /ros2_openvino_toolkit/headposes_estimation ( people_msgs::msg::HeadPoseStamped ) Object Detection: /ros2_openvino_toolkit/detected_objects ( object_msgs::msg::ObjectsInBoxes ) Object Segmentation: /ros2_openvino_toolkit/segmented_obejcts ( people_msgs::msg::ObjectsInMasks ) Rviz Output: /ros2_openvino_toolkit/image_rviz ( sensor_msgs::msg::Image ) Service Object Detection Service: /detect_object ( object_msgs::srv::DetectObject ) Face Detection Service: /detect_face ( object_msgs::srv::DetectObject ) Age & Gender Detection Service: /detect_age_gender ( people_msgs::srv::AgeGender ) Headpose Detection Service: /detect_head_pose ( people_msgs::srv::HeadPose ) Emotion Detection Service: /detect_emotion ( people_msgs::srv::Emotion ) face detection input from image object detection input from realsense camera object segmentation input from video Person Reidentification input from standard camera 4. Known issues -- 5. TODO Support result filtering for inference process, so that the inference results can be filtered to different subsidiary inference. For example, given an image, firstly we do Object Detection on it, secondly we pass cars to vehicle brand recognition and pass license plate to license number recognition. Design resource manager to better use such resources as models, engines, and other external plugins. Develop GUI based configuration and management tools (and monitoring and diagnose tools), in order to provide easy entry for end users to simplify their operation. navigation2 1. Overview The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way. 2. Running the demo The nav2_bringup package is an example bringup system for navigation2 applications. Notes: (December 2018, Crystal Release) We recommend doing this on a Ubuntu 18.04 installation. We\u2019re currently having build issues on 16.04. (see https://github.com/ros-planning/navigation2/issues/353) It is recommended to start with simulation using Gazebo before proceeding to run on a physical robot Launch Navigation2 in simulation with Gazebo (first time users) Terminal 1: Launch Gazebo and Rviz2 Example: See turtlebot3_gazebo models for details export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:<full/path/to/my_robot/models> ros2 launch nav2_bringup gazebo_rviz2_launch.py world:=<full/path/to/gazebo.world> Terminal 2: Launch your robot specific transforms Example: See turtlebot3_gazebo for details ros2 launch turtlebot3_bringup burger_state_publisher.launch.py Terminal 3: Launch map_server and AMCL # Set the tf publisher node to use simulation time or AMCL won't get the transforms correctly ros2 param set /robot_state_publisher use_sim_time True # Launch map_server and AMCL, set map_type as \"occupancy\" by default. ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=<full/path/to/map.yaml> map_type:=occupancy use_sim_time:=True In RVIZ: Make sure all transforms from odom are present. (odom->base_link->base_scan) Localize the robot using \u201c2D Pose Estimate\u201d button. Terminal 4: Run the rest of the Navigation2 bringup ros2 launch nav2_bringup nav2_bringup_2nd_launch.py use_sim_time:=True Terminal 5: Set the World Model and the two costmap nodes to use simulation time ros2 param set /world_model use_sim_time True ros2 param set /global_costmap/global_costmap use_sim_time True ros2 param set /local_costmap/local_costmap use_sim_time True Notes: Setting use_sim_time has to be done dynamically after the nodes are up due to this bug:https://github.com/ros2/rclcpp/issues/595 Sim time needs to be set in every namespace individually. Sometimes setting use_sim_time a second time is required for all the nodes to get updated IF you continue to see WARN messages like the ones below, retry setting the use_sim_time parameter [WARN] [world_model]: Costmap2DROS transform timeout. Current time: 1543616767.1026, global_pose stamp: 758.8040, tolerance: 0.3000, difference: 1543616008.2986 [WARN] [FollowPathNode]: Costmap2DROS transform timeout. Current time: 1543616767.2787, global_pose stamp: 759.0040, tolerance: 0.3000, difference: 1543616008.2747 In RVIZ: Add \"map\" to subscribe topic \"/map\" Add \"RobotModel\", set \"Description Source\" with \"File\", set \"Description File\" with the name of the urdf file for your robot (example: turtlebot3_burger.urdf)\" Localize the robot using \u201c2D Pose Estimate\u201d button. Send the robot a goal using \u201c2D Nav Goal\u201d button. Launch Navigation2 on a Robot (first time users) Pre-requisites: Run SLAM or Cartographer with tele-op to drive the robot and generate a map of an area for testing first. The directions below assume this has already been done. If not, it can be done in ROS1 before beginning to install our code. Publish all the transforms from your robot from base_link to base_scan Launch the code using this launch file and your map.yaml: ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=<full/path/to/map.yaml> map_type:=occupancy In another terminal, run RVIZ: ros2 run rviz2 rviz2 In RVIZ: Make sure all transforms from odom are present. (odom->base_link->base_scan) Localize the robot using \u201c2D Pose Estimate\u201d button. Run the rest of the Navigation2 bringup ros2 launch nav2_bringup nav2_bringup_2nd_launch.py In RVIZ: Localize the robot using \u201c2D Pose Estimate\u201d button. Send the robot a goal using \u201c2D Nav Goal\u201d button. Advanced 1-step Launch for experienced users Pre-requisites: You've completed bringup of your robot successfully following the 2-step process above You know your transforms are being published correctly and AMCL can localize Follow directions above except Instead of running the nav2_bringup_1st_launch.py then the nav2_bringup_2nd_launch.py You can do it in one step like this: ros2 launch nav2_bringup nav2_bringup_launch.py map:=<full/path/to/map.yaml> If running in simulation: ros2 launch nav2_bringup nav2_bringup_launch.py map:=<full/path/to/map.yaml> use_sim_time:=True ros2 param set /world_model use_sim_time True; ros2 param set /global_costmap/global_costmap use_sim_time True; ros2 param set /local_costmap/local_costmap use_sim_time True 3. Knowing issues This stack and ROS2 are still in heavy development and there are some bugs and stability issues being worked on, so please do not try this on a robot without taking heavy safety precautions. THE ROBOT MAY CRASH! 4. TODO Add configuration files for the example bringup Support a more complete map for system level testing","title":"Package Instruction"},{"location":"package_instruction/#quick-start","text":"NOTE: Here we just provide a streamlined version to each projects, if you want to learn the details, please refer to their homepages. NOTE: All projects are depend on Intel Reanlsense cameras. If use other RGB-D sensers, you may need to modify the source code according to instructions as follows.","title":"Quick Start"},{"location":"package_instruction/#ros2_intel_realsense","text":"","title":"ros2_intel_realsense"},{"location":"package_instruction/#1-overview","text":"These are packages for using Intel RealSense cameras (D400 series) with ROS2.","title":"1 Overview"},{"location":"package_instruction/#2-running-the-demo","text":"","title":"2 Running the demo"},{"location":"package_instruction/#21-start-the-camera-node","text":"To start the camera node in ROS2, plug in the camera, then type the following command: # To launch with \"ros2 run\" $ ros2 run realsense_ros2_camera realsense_ros2_camera # OR, to invoke the executable directly $ realsense_ros2_camera This will stream all camera sensors and publish on the appropriate ROS2 topics. PointCloud2 is enabled by default, till we provide ROS2 python launch options.","title":"2.1 Start the camera node"},{"location":"package_instruction/#22-view-camera-data","text":"# in Terminal #1 launch realsense_ros2_camera $ source ~/ros2_ws/install/local_setup.bash $ realsense_ros2_camera # in terminal #2 launch rviz2 $ source ~/ros2_ws/install/local_setup.bash $ rviz2 This will launch RViz2 and display the five streams: color, depth, infra1, infra2, pointcloud. NOTE: Visulization in ROS2 pending on rviz2 .","title":"2.2 View camera data"},{"location":"package_instruction/#3-interfaces","text":"/camera/depth/image_rect_raw /camera/color/image_raw /camera/infra1/image_rect_raw /camera/infra2/image_rect_raw /camera/depth/color/points","title":"3 Interfaces"},{"location":"package_instruction/#4-known-issues","text":"This ROS2 node does not currently provide any dynamic reconfigure support for camera properties/presets. We support Ubuntu Linux Xenial Xerus 16.04 on 64-bit, but not support Mac OS X 10.12 (Sierra) and Windows 10 yet.","title":"4 Known Issues"},{"location":"package_instruction/#5-todo","text":"A few features to be ported from the latest realsense_ros_camera v2.0.2 RGB-D point cloud (depth_registered) Preset/Controls","title":"5 TODO"},{"location":"package_instruction/#ros2_object_analytic","text":"","title":"ros2_object_analytic"},{"location":"package_instruction/#1-overview_1","text":"Object Analytics (OA) is ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. It consumes sensor_msgs::PointClould2 data delivered by RGB-D camera, publishing topics on object detection , object tracking , and object localization in 3D camera coordination system. OA keeps integrating with various \"state-of-the-art\" algorithms. Object detection offload to VPU, Intel Movidius NCS, with MobileNet SSD model and Caffe framework.","title":"1 Overview"},{"location":"package_instruction/#2-running-the-demo_1","text":"","title":"2 Running the demo"},{"location":"package_instruction/#step-1-in-terminal-1-launch-realsense-camera-node","text":"# Terminal 1: . <install-space-with-realsense-ros2-camera>/local_setup.bash realsense_ros2_camera","title":"Step 1. [In terminal 1] Launch Realsense camera node"},{"location":"package_instruction/#step-2-in-terminal-1-launch-ncs-and-oa-node","text":"# Terminal 2 . <install-space-with-object-analytics-launch>/local_setup.bash echo -e \"param_file: mobilenetssd.yaml\\ninput_topic: /object_analytics/rgb\" > `ros2 pkg prefix movidius_ncs_launch`/share/movidius_ncs_launch/config/default.yaml launch `ros2 pkg prefix object_analytics_launch`/share/object_analytics_launch/launch/analytics_movidius_ncs.py","title":"Step 2. [In terminal 1] Launch NCS and OA node"},{"location":"package_instruction/#step-3-in-terminal-1-launch-oa-rviz","text":"# Terminal 3 . <install-space-with-object-analytics-launch>/local_setup.bash launch `ros2 pkg prefix object_analytics_launch`/share/object_analytics_launch/launch/object_rviz.py","title":"Step 3. [In terminal 1] Launch OA Rviz"},{"location":"package_instruction/#3-interfaces_1","text":"","title":"3 Interfaces"},{"location":"package_instruction/#31-subscribed-topics","text":"/movidius_ncs_stream/detected_objects ( object_msgs::msg::ObjectsInBoxes )","title":"3.1 Subscribed topics"},{"location":"package_instruction/#32-published-topics","text":"/object_analytics/rgb ( sensor_msgs::msg::Image ) /object_analytics/pointcloud ( sensor_msgs::msg::PointCloud2 ) /object_analytics/localization ( object_analytics_msgs::msg::ObjectsInBoxes3D ) /object_analytics/tracking ( object_analytics_msgs::msg::TrackedObjects )","title":"3.2 Published topics"},{"location":"package_instruction/#33-customize-launch","text":"By default, object analytics will launch both tracking and localization features, but either tracking or localization or both can be dropped. Detailed please refer comments embedded in launch file.","title":"3.3 Customize launch"},{"location":"package_instruction/#4-known-issues_1","text":"--","title":"4 Known issues"},{"location":"package_instruction/#5-todo_1","text":"-","title":"5 TODO"},{"location":"package_instruction/#ros2_openvino_toolkit","text":"","title":"ros2_openvino_toolkit"},{"location":"package_instruction/#1-overview_2","text":"The OpenVINO\u2122 (Open visual inference and neural network optimization) toolkit provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference. By leveraging Intel\u00ae OpenVINO\u2122 toolkit and corresponding libraries, this runtime framework extends workloads across Intel\u00ae hardware (including accelerators) and maximizes performance. Enables CNN-based deep learning inference at the edge Supports heterogeneous execution across computer vision accelerators\u2014CPU, GPU, Intel\u00ae Movidius\u2122 Neural Compute Stick, and FPGA\u2014using a common API Speeds up time to market via a library of functions and preoptimized kernels Includes optimized calls for OpenCV and OpenVX*","title":"1. Overview"},{"location":"package_instruction/#2-running-the-demo_2","text":"run face detection sample code input from StandardCamera. ros2 launch dynamic_vino_sample pipeline_people_oss.launch.py run face detection sample code input from Image. ros2 launch dynamic_vino_sample pipeline_image_oss.launch.py run object detection sample code input from RealSenseCamera. ros2 launch dynamic_vino_sample pipeline_object_oss.launch.py run object detection sample code input from RealSenseCameraTopic. ros2 launch dynamic_vino_sample pipeline_object_oss_topic.launch.py run object segmentation sample code input from RealSenseCameraTopic. ros2 launch dynamic_vino_sample pipeline_segmentation.launch.py run object segmentation sample code input from Video. ros2 launch dynamic_vino_sample pipeline_video.launch.py run object detection service sample code input from Image Run image processing service: ros2 launch dynamic_vino_sample image_object_server_oss.launch.py Run example application with an absolute path of an image on another console: ros2 run dynamic_vino_sample image_object_client ~/Pictures/car.png","title":"2. Running the demo"},{"location":"package_instruction/#3-interfaces_2","text":"","title":"3. Interfaces"},{"location":"package_instruction/#subscribed-topic","text":"Image topic: /openvino_toolkit/image_raw ( sensor_msgs::msg::Image )","title":"Subscribed Topic"},{"location":"package_instruction/#published-topic","text":"Face Detection: /ros2_openvino_toolkit/face_detection ( object_msgs::msg::ObjectsInBoxes ) Emotion Recognition: /ros2_openvino_toolkit/emotions_recognition ( people_msgs::msg::EmotionsStamped ) Age and Gender Recognition: /ros2_openvino_toolkit/age_genders_Recognition ( people_msgs::msg::AgeGenderStamped ) Head Pose Estimation: /ros2_openvino_toolkit/headposes_estimation ( people_msgs::msg::HeadPoseStamped ) Object Detection: /ros2_openvino_toolkit/detected_objects ( object_msgs::msg::ObjectsInBoxes ) Object Segmentation: /ros2_openvino_toolkit/segmented_obejcts ( people_msgs::msg::ObjectsInMasks ) Rviz Output: /ros2_openvino_toolkit/image_rviz ( sensor_msgs::msg::Image )","title":"Published Topic"},{"location":"package_instruction/#service","text":"Object Detection Service: /detect_object ( object_msgs::srv::DetectObject ) Face Detection Service: /detect_face ( object_msgs::srv::DetectObject ) Age & Gender Detection Service: /detect_age_gender ( people_msgs::srv::AgeGender ) Headpose Detection Service: /detect_head_pose ( people_msgs::srv::HeadPose ) Emotion Detection Service: /detect_emotion ( people_msgs::srv::Emotion ) face detection input from image object detection input from realsense camera object segmentation input from video Person Reidentification input from standard camera","title":"Service"},{"location":"package_instruction/#4-known-issues_2","text":"--","title":"4. Known issues"},{"location":"package_instruction/#5-todo_2","text":"Support result filtering for inference process, so that the inference results can be filtered to different subsidiary inference. For example, given an image, firstly we do Object Detection on it, secondly we pass cars to vehicle brand recognition and pass license plate to license number recognition. Design resource manager to better use such resources as models, engines, and other external plugins. Develop GUI based configuration and management tools (and monitoring and diagnose tools), in order to provide easy entry for end users to simplify their operation.","title":"5. TODO"},{"location":"package_instruction/#navigation2","text":"","title":"navigation2"},{"location":"package_instruction/#1-overview_3","text":"The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way.","title":"1. Overview"},{"location":"package_instruction/#2-running-the-demo_3","text":"The nav2_bringup package is an example bringup system for navigation2 applications. Notes: (December 2018, Crystal Release) We recommend doing this on a Ubuntu 18.04 installation. We\u2019re currently having build issues on 16.04. (see https://github.com/ros-planning/navigation2/issues/353) It is recommended to start with simulation using Gazebo before proceeding to run on a physical robot","title":"2. Running the demo"},{"location":"package_instruction/#launch-navigation2-in-simulation-with-gazebo-first-time-users","text":"","title":"Launch Navigation2 in simulation with Gazebo (first time users)"},{"location":"package_instruction/#terminal-1-launch-gazebo-and-rviz2","text":"Example: See turtlebot3_gazebo models for details export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:<full/path/to/my_robot/models> ros2 launch nav2_bringup gazebo_rviz2_launch.py world:=<full/path/to/gazebo.world>","title":"Terminal 1: Launch Gazebo and Rviz2"},{"location":"package_instruction/#terminal-2-launch-your-robot-specific-transforms","text":"Example: See turtlebot3_gazebo for details ros2 launch turtlebot3_bringup burger_state_publisher.launch.py","title":"Terminal 2: Launch your robot specific transforms"},{"location":"package_instruction/#terminal-3-launch-map_server-and-amcl","text":"# Set the tf publisher node to use simulation time or AMCL won't get the transforms correctly ros2 param set /robot_state_publisher use_sim_time True # Launch map_server and AMCL, set map_type as \"occupancy\" by default. ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=<full/path/to/map.yaml> map_type:=occupancy use_sim_time:=True In RVIZ: Make sure all transforms from odom are present. (odom->base_link->base_scan) Localize the robot using \u201c2D Pose Estimate\u201d button.","title":"Terminal 3: Launch map_server and AMCL"},{"location":"package_instruction/#terminal-4","text":"Run the rest of the Navigation2 bringup ros2 launch nav2_bringup nav2_bringup_2nd_launch.py use_sim_time:=True","title":"Terminal 4:"},{"location":"package_instruction/#terminal-5","text":"Set the World Model and the two costmap nodes to use simulation time ros2 param set /world_model use_sim_time True ros2 param set /global_costmap/global_costmap use_sim_time True ros2 param set /local_costmap/local_costmap use_sim_time True Notes: Setting use_sim_time has to be done dynamically after the nodes are up due to this bug:https://github.com/ros2/rclcpp/issues/595 Sim time needs to be set in every namespace individually. Sometimes setting use_sim_time a second time is required for all the nodes to get updated IF you continue to see WARN messages like the ones below, retry setting the use_sim_time parameter [WARN] [world_model]: Costmap2DROS transform timeout. Current time: 1543616767.1026, global_pose stamp: 758.8040, tolerance: 0.3000, difference: 1543616008.2986 [WARN] [FollowPathNode]: Costmap2DROS transform timeout. Current time: 1543616767.2787, global_pose stamp: 759.0040, tolerance: 0.3000, difference: 1543616008.2747 In RVIZ: Add \"map\" to subscribe topic \"/map\" Add \"RobotModel\", set \"Description Source\" with \"File\", set \"Description File\" with the name of the urdf file for your robot (example: turtlebot3_burger.urdf)\" Localize the robot using \u201c2D Pose Estimate\u201d button. Send the robot a goal using \u201c2D Nav Goal\u201d button.","title":"Terminal 5:"},{"location":"package_instruction/#launch-navigation2-on-a-robot-first-time-users","text":"Pre-requisites: Run SLAM or Cartographer with tele-op to drive the robot and generate a map of an area for testing first. The directions below assume this has already been done. If not, it can be done in ROS1 before beginning to install our code. Publish all the transforms from your robot from base_link to base_scan Launch the code using this launch file and your map.yaml: ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=<full/path/to/map.yaml> map_type:=occupancy In another terminal, run RVIZ: ros2 run rviz2 rviz2 In RVIZ: Make sure all transforms from odom are present. (odom->base_link->base_scan) Localize the robot using \u201c2D Pose Estimate\u201d button. Run the rest of the Navigation2 bringup ros2 launch nav2_bringup nav2_bringup_2nd_launch.py In RVIZ: Localize the robot using \u201c2D Pose Estimate\u201d button. Send the robot a goal using \u201c2D Nav Goal\u201d button.","title":"Launch Navigation2 on a Robot (first time users)"},{"location":"package_instruction/#advanced-1-step-launch-for-experienced-users","text":"Pre-requisites: You've completed bringup of your robot successfully following the 2-step process above You know your transforms are being published correctly and AMCL can localize Follow directions above except Instead of running the nav2_bringup_1st_launch.py then the nav2_bringup_2nd_launch.py You can do it in one step like this: ros2 launch nav2_bringup nav2_bringup_launch.py map:=<full/path/to/map.yaml> If running in simulation: ros2 launch nav2_bringup nav2_bringup_launch.py map:=<full/path/to/map.yaml> use_sim_time:=True ros2 param set /world_model use_sim_time True; ros2 param set /global_costmap/global_costmap use_sim_time True; ros2 param set /local_costmap/local_costmap use_sim_time True","title":"Advanced 1-step Launch for experienced users"},{"location":"package_instruction/#3-knowing-issues","text":"This stack and ROS2 are still in heavy development and there are some bugs and stability issues being worked on, so please do not try this on a robot without taking heavy safety precautions. THE ROBOT MAY CRASH!","title":"3. Knowing issues"},{"location":"package_instruction/#4-todo","text":"Add configuration files for the example bringup Support a more complete map for system level testing","title":"4. TODO"},{"location":"preparation/","text":"Preparation Hardware An x86_64 computer running Ubuntu 18.04. NOTE: OS X and Windows are not supported yet Intel\u00ae RealSense\u2122 Devices \u2122 Neural Compute Stick ROBOT SDK Robot SDK Robot SDK could install all software components. For details please refer to https://github.intel.com/otc-rse/robot_sdk.","title":"Preparation"},{"location":"preparation/#preparation","text":"","title":"Preparation"},{"location":"preparation/#hardware","text":"An x86_64 computer running Ubuntu 18.04. NOTE: OS X and Windows are not supported yet Intel\u00ae RealSense\u2122 Devices \u2122 Neural Compute Stick","title":"Hardware"},{"location":"preparation/#robot-sdk","text":"Robot SDK Robot SDK could install all software components. For details please refer to https://github.intel.com/otc-rse/robot_sdk.","title":"ROBOT SDK"}]}